{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/carlyryan/CIS450/blob/main/Cozy_with_the_Good_Eats_Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqDcfFDuU-56"
   },
   "source": [
    "Our project uses two datasets:\n",
    "1. A [Yelp](https://www.yelp.com/dataset) dataset: The Yelp dataset covers 160,585 businesses across 8 metropolitan areas. There are a couple of different tables: business, review, user, checkin, tip, and photo. We’ll probably focus a lot of our attention on the business and review tables. (--> nothing in NYC https://github.com/Yelp/dataset-examples/issues/47)\n",
    "\n",
    "2. An [Airbnb](http://insideairbnb.com/get-the-data.html) dataset: The AIrBnB dataset, provided by ‘Inside Airbnb’, covers 36,923 locations across the entire NYC metropolitan area. Important columns that we have are latitude and longitude, number of bedrooms, as well as review statistics for a location etc. \n",
    "\n",
    "\n",
    "To prepare the datasets for ingestion into the database, we want to:\n",
    "1. Clean missing / misentered values\n",
    "2. Detect and solve entity resolution problems\n",
    "3. Replace categorical variables with numeric indicators for efficiency\n",
    "4. Export the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w7qcswO1KqC"
   },
   "source": [
    "## Cleaning of Data for Stay cozy with good eats\n",
    "\n",
    "we will look to : \n",
    "*   Removal of rows with missing values\n",
    "*   Replacement of missing values with default values\n",
    "*   Dectection of entity resolution problems\n",
    "*   Simple entity resolution\n",
    "*   Removal of unpaired entities\n",
    "*   Replacement of categorical variables with indicators\n",
    "*   Identification of candidate indices\n",
    "*   Data exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymfuSljIMWdO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import math\n",
    "import requests \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bG0H3MGaNA8M",
    "outputId": "6836b394-de2b-4dfb-961b-369bb426891e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "prefix = '/content/drive'\n",
    "from google.colab import drive\n",
    "drive.mount(prefix, force_remount=True)\n",
    "#success code: 4/1AX4XfWg77OvWTHG6cqauPcN_8-a3FWdeeWTGs1yBFShQYlfue6DcdgYd-70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2fr3ngdPlJ_"
   },
   "source": [
    "Now, copy the paths to the life expectancy dataset and census dataset to the `le_path` and `census_path` variables respectively in the cell below. Then, run the cell to save your variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSpkgOZHPkuE"
   },
   "outputs": [],
   "source": [
    "yelp_path = '/content/drive/Shareddrives/CIS550/raw_datasets/yelp.csv' # we dont use this \n",
    "airbnb_path = '/content/drive/Shareddrives/CIS550/raw_datasets/airbnb.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFyiS_Vocek_",
    "outputId": "33b492d8-ac46-4f7f-dd08-868a0a262a03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,4,7,17,29,49,60,62,79,86,94) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "yelp_raw = pd.read_csv(yelp_path) # we dont use this \n",
    "airbnb_raw = pd.read_csv(airbnb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSZoJnysBila"
   },
   "source": [
    "# Yelp Data Cleanup\n",
    "We want to keep all of the columns so we will just look for missing or misentered values, create categorical values, and check for entity resolution. We also only want New York Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "JLuILSc25bLZ",
    "outputId": "b419ab4d-ac98-477d-a446-abcd33603116"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes.Ambience.divey</th>\n",
       "      <th>attributes.Dietary Restrictions.vegan</th>\n",
       "      <th>attributes.Happy Hour</th>\n",
       "      <th>hours.Thursday.open</th>\n",
       "      <th>attributes.Order at Counter</th>\n",
       "      <th>attributes.Hair Types Specialized In.africanamerican</th>\n",
       "      <th>attributes.Hair Types Specialized In.kids</th>\n",
       "      <th>attributes.BYOB</th>\n",
       "      <th>hours.Friday.open</th>\n",
       "      <th>attributes.Good For.latenight</th>\n",
       "      <th>attributes.Outdoor Seating</th>\n",
       "      <th>attributes.Alcohol</th>\n",
       "      <th>attributes.Ambience.classy</th>\n",
       "      <th>attributes.By Appointment Only</th>\n",
       "      <th>attributes.Parking.lot</th>\n",
       "      <th>business_id</th>\n",
       "      <th>attributes.Ambience.touristy</th>\n",
       "      <th>attributes.Corkage</th>\n",
       "      <th>hours.Tuesday.open</th>\n",
       "      <th>attributes.Good For.brunch</th>\n",
       "      <th>categories</th>\n",
       "      <th>attributes.Waiter Service</th>\n",
       "      <th>hours.Monday.open</th>\n",
       "      <th>name</th>\n",
       "      <th>attributes.Parking.street</th>\n",
       "      <th>attributes.Ambience.hipster</th>\n",
       "      <th>attributes.BYOB/Corkage</th>\n",
       "      <th>attributes.Hair Types Specialized In.straightperms</th>\n",
       "      <th>attributes.Music.live</th>\n",
       "      <th>attributes.Dietary Restrictions.dairy-free</th>\n",
       "      <th>attributes.Music.background_music</th>\n",
       "      <th>attributes.Price Range</th>\n",
       "      <th>attributes.Good For.breakfast</th>\n",
       "      <th>attributes.Parking.garage</th>\n",
       "      <th>attributes.Music.karaoke</th>\n",
       "      <th>attributes.Good For Dancing</th>\n",
       "      <th>review_count</th>\n",
       "      <th>attributes.Hair Types Specialized In.asian</th>\n",
       "      <th>state</th>\n",
       "      <th>attributes.Accepts Credit Cards</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>attributes.Wheelchair Accessible</th>\n",
       "      <th>attributes.Dietary Restrictions.gluten-free</th>\n",
       "      <th>stars</th>\n",
       "      <th>attributes.Dietary Restrictions.kosher</th>\n",
       "      <th>type</th>\n",
       "      <th>attributes.Caters</th>\n",
       "      <th>attributes.Ambience.intimate</th>\n",
       "      <th>latitude</th>\n",
       "      <th>attributes.Good For.dinner</th>\n",
       "      <th>attributes.Coat Check</th>\n",
       "      <th>longitude</th>\n",
       "      <th>hours.Monday.close</th>\n",
       "      <th>attributes.Hair Types Specialized In.extensions</th>\n",
       "      <th>hours.Tuesday.close</th>\n",
       "      <th>hours.Saturday.close</th>\n",
       "      <th>attributes.Good for Kids</th>\n",
       "      <th>attributes.Parking.validated</th>\n",
       "      <th>hours.Sunday.open</th>\n",
       "      <th>attributes.Accepts Insurance</th>\n",
       "      <th>attributes.Music.dj</th>\n",
       "      <th>attributes.Dietary Restrictions.soy-free</th>\n",
       "      <th>attributes.Has TV</th>\n",
       "      <th>hours.Sunday.close</th>\n",
       "      <th>attributes.Ambience.casual</th>\n",
       "      <th>attributes.Hair Types Specialized In.perms</th>\n",
       "      <th>attributes.Dogs Allowed</th>\n",
       "      <th>attributes.Drive-Thru</th>\n",
       "      <th>attributes.Dietary Restrictions.vegetarian</th>\n",
       "      <th>hours.Wednesday.open</th>\n",
       "      <th>attributes.Noise Level</th>\n",
       "      <th>attributes.Smoking</th>\n",
       "      <th>attributes.Attire</th>\n",
       "      <th>attributes.Hair Types Specialized In.curly</th>\n",
       "      <th>attributes.Good For Groups</th>\n",
       "      <th>neighborhoods</th>\n",
       "      <th>attributes.Open 24 Hours</th>\n",
       "      <th>attributes.Ambience.romantic</th>\n",
       "      <th>attributes.Music.jukebox</th>\n",
       "      <th>attributes.Ambience.upscale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>none</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11:00</td>\n",
       "      <td>False</td>\n",
       "      <td>['Fast Food', 'Restaurants']</td>\n",
       "      <td>False</td>\n",
       "      <td>11:00</td>\n",
       "      <td>Mr Hoagie</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Dravosburg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>40.354327</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.900706</td>\n",
       "      <td>21:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11:00</td>\n",
       "      <td>average</td>\n",
       "      <td>NaN</td>\n",
       "      <td>casual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UsFtqoBl7naz8AVUBZMjQQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Nightlife']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clancy's Pub</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Dravosburg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.350553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.886814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3eu6MEFlq2Dg7bQh8QbdOg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Auto Repair', 'Automotive']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joe Cislo's Auto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Dravosburg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.350956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-79.889059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cE27W9VPgO88Qxe4ol6y_g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Active Life', 'Mini Golf', 'Golf']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cool Springs Golf Center</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Bethel Park</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.354116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-80.014660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>HZdLhv6COCleJMo7nPl-RA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Shopping', 'Home Services', 'Internet Servic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10:00</td>\n",
       "      <td>Verizon</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PA</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.357620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-80.059980</td>\n",
       "      <td>21:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21:00</td>\n",
       "      <td>21:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>11:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  attributes.Ambience.divey  ... attributes.Ambience.upscale\n",
       "0                     False  ...                       False\n",
       "1                       NaN  ...                         NaN\n",
       "2                       NaN  ...                         NaN\n",
       "3                       NaN  ...                         NaN\n",
       "4                       NaN  ...                         NaN\n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JFyD6aOCMpN",
    "outputId": "d0b0d9a2-2223-4776-ebb2-c97aa87541c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AL',\n",
       " 'AR',\n",
       " 'AZ',\n",
       " 'BW',\n",
       " 'CA',\n",
       " 'EDH',\n",
       " 'ELN',\n",
       " 'FIF',\n",
       " 'HAM',\n",
       " 'IL',\n",
       " 'KHL',\n",
       " 'MA',\n",
       " 'MLN',\n",
       " 'MN',\n",
       " 'NC',\n",
       " 'NM',\n",
       " 'NTH',\n",
       " 'NV',\n",
       " 'NW',\n",
       " 'ON',\n",
       " 'OR',\n",
       " 'PA',\n",
       " 'QC',\n",
       " 'RP',\n",
       " 'SC',\n",
       " 'SCB',\n",
       " 'TX',\n",
       " 'WI',\n",
       " nan}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = yelp_raw[yelp_raw['state']=='NY']\n",
    "set(yelp_raw['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAE6QNs_aAoN",
    "outputId": "5c9c9086-1278-49af-ead7-fb38735709b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['attributes.Ambience.divey', 'attributes.Dietary Restrictions.vegan',\n",
       "       'attributes.Happy Hour', 'hours.Thursday.open',\n",
       "       'attributes.Order at Counter',\n",
       "       'attributes.Hair Types Specialized In.africanamerican',\n",
       "       'attributes.Hair Types Specialized In.kids', 'attributes.BYOB',\n",
       "       'hours.Friday.open', 'attributes.Good For.latenight',\n",
       "       'attributes.Outdoor Seating', 'attributes.Alcohol',\n",
       "       'attributes.Ambience.classy', 'attributes.By Appointment Only',\n",
       "       'attributes.Parking.lot', 'business_id', 'attributes.Ambience.touristy',\n",
       "       'attributes.Corkage', 'hours.Tuesday.open',\n",
       "       'attributes.Good For.brunch', 'categories', 'attributes.Waiter Service',\n",
       "       'hours.Monday.open', 'name', 'attributes.Parking.street',\n",
       "       'attributes.Ambience.hipster', 'attributes.BYOB/Corkage',\n",
       "       'attributes.Hair Types Specialized In.straightperms',\n",
       "       'attributes.Music.live', 'attributes.Dietary Restrictions.dairy-free',\n",
       "       'attributes.Music.background_music', 'attributes.Price Range',\n",
       "       'attributes.Good For.breakfast', 'attributes.Parking.garage',\n",
       "       'attributes.Music.karaoke', 'attributes.Good For Dancing',\n",
       "       'review_count', 'attributes.Hair Types Specialized In.asian', 'state',\n",
       "       'attributes.Accepts Credit Cards', 'hours.Friday.close',\n",
       "       'attributes.Good For.lunch', 'attributes.Parking.valet',\n",
       "       'attributes.Take-out', 'full_address', 'hours.Thursday.close',\n",
       "       'attributes.Hair Types Specialized In.coloring',\n",
       "       'attributes.Good For.dessert', 'attributes.Music.video',\n",
       "       'attributes.Dietary Restrictions.halal',\n",
       "       'attributes.Takes Reservations', 'hours.Saturday.open',\n",
       "       'attributes.Ages Allowed', 'attributes.Ambience.trendy',\n",
       "       'attributes.Delivery', 'hours.Wednesday.close', 'attributes.Wi-Fi',\n",
       "       'open', 'city', 'attributes.Wheelchair Accessible',\n",
       "       'attributes.Dietary Restrictions.gluten-free', 'stars',\n",
       "       'attributes.Dietary Restrictions.kosher', 'type', 'attributes.Caters',\n",
       "       'attributes.Ambience.intimate', 'latitude',\n",
       "       'attributes.Good For.dinner', 'attributes.Coat Check', 'longitude',\n",
       "       'hours.Monday.close', 'attributes.Hair Types Specialized In.extensions',\n",
       "       'hours.Tuesday.close', 'hours.Saturday.close',\n",
       "       'attributes.Good for Kids', 'attributes.Parking.validated',\n",
       "       'hours.Sunday.open', 'attributes.Accepts Insurance',\n",
       "       'attributes.Music.dj', 'attributes.Dietary Restrictions.soy-free',\n",
       "       'attributes.Has TV', 'hours.Sunday.close', 'attributes.Ambience.casual',\n",
       "       'attributes.Hair Types Specialized In.perms', 'attributes.Dogs Allowed',\n",
       "       'attributes.Drive-Thru', 'attributes.Dietary Restrictions.vegetarian',\n",
       "       'hours.Wednesday.open', 'attributes.Noise Level', 'attributes.Smoking',\n",
       "       'attributes.Attire', 'attributes.Hair Types Specialized In.curly',\n",
       "       'attributes.Good For Groups', 'neighborhoods',\n",
       "       'attributes.Open 24 Hours', 'attributes.Ambience.romantic',\n",
       "       'attributes.Music.jukebox', 'attributes.Ambience.upscale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVSmHu9rVR4e"
   },
   "source": [
    "## Important\n",
    "you can see there is unfortunately no NYC info in the yelp dataset. The following is thus my attempt to grab it directly from the yelp API. It is inspired slightly by the code at the [following link](https://rspiro9.github.io/nyc_restaurant_yelp_and_inspection_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1D7W5munGgF",
    "outputId": "556d5ac1-0bdd-4add-f915-41d0ebd981c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpHmhc_5dZtj"
   },
   "outputs": [],
   "source": [
    "#set(yelp_raw['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k47bprpVv1f"
   },
   "source": [
    "connect to my API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Cdjn2A4kC8W"
   },
   "outputs": [],
   "source": [
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\t\t\t\t\n",
    "# Pull in keys and specifically draw out the api key. I have removed the specific path to the keys \n",
    "# for security purposes:\n",
    "#keys = get_keys(\"/Users/3a0K2dom2OuZm69n7rZMR78to5RL7OQgJWqgqIVnKXbFFcPQ4KgMoNWNPrDZ-DVlVUnGB4YuYOob8xU-Fm1WPaK4vLmbaLQ_ljszj7qUPyXNavWHRp-5OgfnPb6OYXYx/yelp_api.json\")\n",
    "api_key = '3a0K2dom2OuZm69n7rZMR78to5RL7OQgJWqgqIVnKXbFFcPQ4KgMoNWNPrDZ-DVlVUnGB4YuYOob8xU-Fm1WPaK4vLmbaLQ_ljszj7qUPyXNavWHRp-5OgfnPb6OYXYx'\n",
    "\n",
    "# URL to pull data from:\n",
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "# Identify headers:\n",
    "headers = {'Authorization': 'Bearer {}'.format(api_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGDdq-Ro0_27"
   },
   "outputs": [],
   "source": [
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "term = \"Restaurants\"\n",
    "location =  'Manhattan, NY'\n",
    "categories = \"(restaurants, All)\"\n",
    "url_params = {\n",
    "                        'location': location,\n",
    "                        'term' : term,\n",
    "                        'categories': categories,\n",
    "                        'limit': 50,\n",
    "                    }\n",
    "response = requests.get(url, headers=headers, params=url_params)\n",
    "response_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2Tkj05fHeYx",
    "outputId": "22ac1384-a2a2-4155-bca1-8f0590457ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "500 Bad Request on offset 200\n"
     ]
    }
   ],
   "source": [
    "#create empty dataframe for dataset\n",
    "column_names = ['business_id', 'categories', 'name',\n",
    "'postal_code', 'latitude', 'longitude', 'stars',\n",
    "'review_count', 'price_range', 'hours', 'address', 'neighborhood', 'url']\n",
    "restaurant =  pd.DataFrame(columns = column_names)\n",
    "\n",
    "location =  'Manhattan, NY'\n",
    "categories = \"(restaurants, All)\"\n",
    "\n",
    "#looping through different offsets to get restaurants, then I will add to the dataframe\n",
    "for offset in range(0, 1000, 50):\n",
    "  search_limit = 50\n",
    "  term = \"Restaurants\"\n",
    "  url_params = {\n",
    "                  'location': location,\n",
    "                  'term' : term,\n",
    "                  'limit': search_limit,\n",
    "                  'categories': categories,\n",
    "                  'offset': offset,\n",
    "              }\n",
    "  response = requests.get(url, headers=headers, params=url_params)\n",
    "  if response.status_code == 400:\n",
    "            print(f'400 Bad Request on offset {offset}')\n",
    "            break\n",
    "  if response.status_code == 500:\n",
    "            print(f'500 Bad Request on offset {offset}')\n",
    "            break\n",
    "  response_json = response.json()\n",
    "  print(response.status_code)\n",
    "  for i in range(50):\n",
    "    this_business = response_json['businesses'][i]\n",
    "    try:\n",
    "      price = this_business['price'] \n",
    "    except:\n",
    "      price = 'UNKNOWN'\n",
    "    #price = this_business['price'] if this_business['price'] else 'UNKNOWN'\n",
    "    new_row = {'business_id': this_business['id'], \n",
    "               'categories':this_business['categories'][0]['alias'],\n",
    "               'name':this_business['name'],\n",
    "               'postal_code':this_business['location']['zip_code'],\n",
    "               'latitude': this_business['coordinates']['latitude'],\n",
    "               'longitude':this_business['coordinates']['longitude'],\n",
    "               'stars':this_business['rating'],\n",
    "               'review_count': this_business['review_count'],\n",
    "               'price_range': price, #'hours': ,\n",
    "               'address': this_business['location']['address1'], #'neighborhood': , \n",
    "               'url': this_business['url']}\n",
    "    restaurant = restaurant.append(new_row, ignore_index=True)\n",
    "  #print(response_json['businesses'][0]['name'], response_json, response)\n",
    "  #print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "lI-bVSUCYNIs",
    "outputId": "58b87d11-1ff0-45a2-e9df-a1cbb1ba7462"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>price_range</th>\n",
       "      <th>hours</th>\n",
       "      <th>address</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DGhWO1sUWydVeR5j5ZZaMw</td>\n",
       "      <td>french</td>\n",
       "      <td>La Grande Boucherie</td>\n",
       "      <td>10019</td>\n",
       "      <td>40.762627</td>\n",
       "      <td>-73.980841</td>\n",
       "      <td>4.5</td>\n",
       "      <td>602</td>\n",
       "      <td>$$$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145 W 53rd St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.yelp.com/biz/la-grande-boucherie-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VvsZAnEwU4c8Xkyrzx05Nw</td>\n",
       "      <td>korean</td>\n",
       "      <td>Anytime</td>\n",
       "      <td>10001</td>\n",
       "      <td>40.747763</td>\n",
       "      <td>-73.986779</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1707</td>\n",
       "      <td>$$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23 W 32nd St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.yelp.com/biz/anytime-new-york?adju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B3_K2kUVbYOU0VaLcj_LTw</td>\n",
       "      <td>thai</td>\n",
       "      <td>Thai Villa</td>\n",
       "      <td>10003</td>\n",
       "      <td>40.739020</td>\n",
       "      <td>-73.990650</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3879</td>\n",
       "      <td>$$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5 E 19th St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.yelp.com/biz/thai-villa-new-york-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J3NT61-AH5d5Gu5tFJhYSw</td>\n",
       "      <td>newamerican</td>\n",
       "      <td>The Cabin NYC</td>\n",
       "      <td>10009</td>\n",
       "      <td>40.723930</td>\n",
       "      <td>-73.983830</td>\n",
       "      <td>4.0</td>\n",
       "      <td>351</td>\n",
       "      <td>$$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 E 4th St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.yelp.com/biz/the-cabin-nyc-new-yor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN3mOWF5e_pnR1ArqM2bHQ</td>\n",
       "      <td>vietnamese</td>\n",
       "      <td>OBAO</td>\n",
       "      <td>10036</td>\n",
       "      <td>40.760654</td>\n",
       "      <td>-73.991332</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2853</td>\n",
       "      <td>$$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>647 9th Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.yelp.com/biz/obao-new-york-3?adjus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  ...                                                url\n",
       "0  DGhWO1sUWydVeR5j5ZZaMw  ...  https://www.yelp.com/biz/la-grande-boucherie-n...\n",
       "1  VvsZAnEwU4c8Xkyrzx05Nw  ...  https://www.yelp.com/biz/anytime-new-york?adju...\n",
       "2  B3_K2kUVbYOU0VaLcj_LTw  ...  https://www.yelp.com/biz/thai-villa-new-york-2...\n",
       "3  J3NT61-AH5d5Gu5tFJhYSw  ...  https://www.yelp.com/biz/the-cabin-nyc-new-yor...\n",
       "4  NN3mOWF5e_pnR1ArqM2bHQ  ...  https://www.yelp.com/biz/obao-new-york-3?adjus...\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant.head()\n",
    "restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpZzH50ldj2r"
   },
   "outputs": [],
   "source": [
    "#create empty dataframe for dataset\n",
    "column_names = ['review_id',\n",
    "'stars',\n",
    "'date',\n",
    "'useful',\n",
    "'funny',\n",
    "'cool',\n",
    "'review_of']\n",
    "review =  pd.DataFrame(columns = column_names)\n",
    "\n",
    "url = 'https://api.yelp.com/v3/businesses/{id}/reviews'\n",
    "#loop through restaurants in df\n",
    "for restaurant_id in restaurant['business_id']: \n",
    "  search_limit = 50\n",
    "  url = 'https://api.yelp.com/v3/businesses/' + restaurant_id + '/reviews'\n",
    "  url_params = {\n",
    "                'limit': search_limit,\n",
    "              }\n",
    "  response = requests.get(url, headers=headers, params=url_params)\n",
    "  if response.status_code == 400:\n",
    "            print(f'400 Bad Request')\n",
    "            break\n",
    "  if response.status_code == 500:\n",
    "            print(f'500 Bad Request ')\n",
    "            break\n",
    "  response_json = response.json()\n",
    "  for i in range(len(response_json['reviews'])):\n",
    "    this_review = response_json['reviews'][i]\n",
    "    #price = this_business['price'] if this_business['price'] else 'UNKNOWN'\n",
    "    new_row = { ADD HERE}\n",
    "    review = review.append(new_row, ignore_index=True)\n",
    "  #print(response_json['businesses'][0]['name'], response_json, response)\n",
    "  #print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hl-eW3T_eAWl",
    "outputId": "318909d5-2e8f-41cf-a24e-86e8b597c825"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'possible_languages': ['en', 'es', 'fr'],\n",
       " 'reviews': [{'id': 'N3weP146Gwqe4k3Jxj-Viw',\n",
       "   'rating': 5,\n",
       "   'text': 'Beautiful place that is just amazing and  gigantic.  Good ambiance with a  tremendous bar scene.  Great for people watching and you can meet really super...',\n",
       "   'time_created': '2021-10-19 20:15:46',\n",
       "   'url': 'https://www.yelp.com/biz/la-grande-boucherie-new-york-2?adjust_creative=LfJ5XuLXZYLlnozpquhGdA&hrid=N3weP146Gwqe4k3Jxj-Viw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=LfJ5XuLXZYLlnozpquhGdA',\n",
       "   'user': {'id': 'X87YR8YLa2jiuGmr8oCNjA',\n",
       "    'image_url': 'https://s3-media1.fl.yelpcdn.com/photo/l_mxRdr7by15xqzJKJN-gA/o.jpg',\n",
       "    'name': 'Antonio P.',\n",
       "    'profile_url': 'https://www.yelp.com/user_details?userid=X87YR8YLa2jiuGmr8oCNjA'}},\n",
       "  {'id': '4fsnqFUDN2Xql0xGxIPTbw',\n",
       "   'rating': 5,\n",
       "   'text': 'Such amazing combination of a beautiful ambiance and menu. Highly recommended. The drinks were fantastic, with exquisite food. Our server was very attentive...',\n",
       "   'time_created': '2021-10-25 16:30:13',\n",
       "   'url': 'https://www.yelp.com/biz/la-grande-boucherie-new-york-2?adjust_creative=LfJ5XuLXZYLlnozpquhGdA&hrid=4fsnqFUDN2Xql0xGxIPTbw&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=LfJ5XuLXZYLlnozpquhGdA',\n",
       "   'user': {'id': 'tuZ1Kbf7RYvCJwdC393jpA',\n",
       "    'image_url': None,\n",
       "    'name': 'Cara B.',\n",
       "    'profile_url': 'https://www.yelp.com/user_details?userid=tuZ1Kbf7RYvCJwdC393jpA'}},\n",
       "  {'id': 'NtVY6SeheH9kk8tgNBxgJg',\n",
       "   'rating': 5,\n",
       "   'text': 'My 4th time coming to La Grande Boucherie, decided to have a last minute dinner with my best friend. Today, I was served by Sanja. She showed me great...',\n",
       "   'time_created': '2021-10-01 19:36:50',\n",
       "   'url': 'https://www.yelp.com/biz/la-grande-boucherie-new-york-2?adjust_creative=LfJ5XuLXZYLlnozpquhGdA&hrid=NtVY6SeheH9kk8tgNBxgJg&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_reviews&utm_source=LfJ5XuLXZYLlnozpquhGdA',\n",
       "   'user': {'id': 'pE5KwE7810rYle2qz6YAhw',\n",
       "    'image_url': None,\n",
       "    'name': 'Iris G.',\n",
       "    'profile_url': 'https://www.yelp.com/user_details?userid=pE5KwE7810rYle2qz6YAhw'}}],\n",
       " 'total': 602}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://api.yelp.com/v3/businesses/' + 'DGhWO1sUWydVeR5j5ZZaMw' + '/reviews'\n",
    "\n",
    "search_limit = 50\n",
    "url_params = {\n",
    "                'limit': search_limit,\n",
    "}\n",
    "response = requests.get(url, headers=headers, params=url_params)\n",
    "response_json = response.json()\n",
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjX17CWRYU5T"
   },
   "source": [
    "## What we need now:\n",
    "1. decide if we want reviews table and import that\n",
    "2. figure out hours and neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbjcwLlSKPPt"
   },
   "source": [
    "# Airbnb Data Cleanup\n",
    "For the Airbnb Data, we want to remove a lot of columns and only keep a few. We also want to have a host table. \n",
    "\n",
    "We will also look for missing or misentered values, create categorical values, and check for entity resolution. We also only want New York Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMZGJ83-5Z2X"
   },
   "outputs": [],
   "source": [
    "airbnb_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnH5Fhe--BSt"
   },
   "source": [
    "### Colums that the Airbnb csv started with : \n",
    "\n",
    "```\n",
    "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'name', 'description',\n",
    "       'neighborhood_overview', 'picture_url', 'host_id', 'host_url',\n",
    "       'host_name', 'host_since', 'host_location', 'host_about',\n",
    "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
    "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
    "       'host_neighbourhood', 'host_listings_count',\n",
    "       'host_total_listings_count', 'host_verifications',\n",
    "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
    "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
    "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
    "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
    "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
    "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
    "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
    "       'availability_30', 'availability_60', 'availability_90',\n",
    "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
    "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
    "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
    "       'review_scores_cleanliness', 'review_scores_checkin',\n",
    "       'review_scores_communication', 'review_scores_location',\n",
    "       'review_scores_value', 'license', 'instant_bookable',\n",
    "       'calculated_host_listings_count',\n",
    "       'calculated_host_listings_count_entire_homes',\n",
    "       'calculated_host_listings_count_private_rooms',\n",
    "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
    "      dtype='object')\n",
    "```\n",
    "\n",
    "### Our DDL: \n",
    "```\n",
    "CREATE TABLE Airbnb(\n",
    "Id int PRIMARY KEY,\n",
    "Listing_url varchar(20),\n",
    "Latitude float,\n",
    "Longitude float,\n",
    "bathrooms float, \n",
    "bedrooms int,\n",
    "price float, \n",
    "review_scores_value float, \n",
    "review_scores_count int, \n",
    "neighborhood varchar(40))\n",
    "foreign key host_id references Host.host_id\n",
    "\n",
    "CREATE TABLE Host(\n",
    "host_id int PRIMARY KEY,\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Columns we would like to thus keep: \n",
    "```\n",
    "Airbnb: [\n",
    "'id', 'listing_url', 'name', 'description',\n",
    "       'neighborhood_overview', 'picture_url', 'host_id'\n",
    "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
    "       'longitude', 'property_type', 'room_type', 'accommodates',\n",
    "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
    "       'minimum_nights', 'maximum_nights', 'number_of_reviews', 'review_scores_rating',\n",
    "       'review_scores_cleanliness', 'review_scores_checkin',\n",
    "       'review_scores_communication', 'review_scores_location', 'instant_bookable']\n",
    "# id is the primary key\n",
    "# host_id is a foreign key to Host\n",
    "\n",
    "Host: [\n",
    "  'host_id', 'host_url',\n",
    "       'host_name', 'host_since', 'host_location', 'host_about',\n",
    "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
    "       'host_is_superhost',\n",
    "       'host_neighbourhood', \n",
    "       'host_total_listings_count'\n",
    "]\n",
    "# host_id is the primary key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG32wiuAM-dX"
   },
   "outputs": [],
   "source": [
    "airbnb = airbnb_raw[[\n",
    "'id', 'listing_url', 'name', 'description',\n",
    "       'neighborhood_overview', 'picture_url', 'host_id',\n",
    "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
    "       'longitude', 'property_type', 'room_type', 'accommodates',\n",
    "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
    "       'minimum_nights', 'maximum_nights', 'number_of_reviews', 'review_scores_rating',\n",
    "       'review_scores_cleanliness', 'review_scores_checkin',\n",
    "       'review_scores_communication', 'review_scores_location', 'instant_bookable']]\n",
    "host = airbnb_raw[[\n",
    "  'host_id', 'host_url',\n",
    "       'host_name', 'host_since', 'host_location', 'host_about',\n",
    "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
    "       'host_is_superhost',\n",
    "       'host_neighbourhood', \n",
    "       'host_total_listings_count'\n",
    "]]\n",
    "host.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQkYxhBgddNu"
   },
   "source": [
    "#Cleaning Host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8RIuk6VP0YZ"
   },
   "outputs": [],
   "source": [
    "set(host.host_response_time)\n",
    "host['host_response_rate'].isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuzwXU9tTt2P"
   },
   "outputs": [],
   "source": [
    "host.iloc[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ldXSnLfW5Il"
   },
   "outputs": [],
   "source": [
    "list_non_null_host_fields = ['host_is_superhost','host_id', 'host_url',\n",
    "       'host_name', 'host_since', 'host_location',\n",
    "       'host_is_superhost',\n",
    "       'host_neighbourhood', \n",
    "       'host_total_listings_count', 'host_response_rate', \n",
    "       'host_acceptance_rate']\n",
    "missing_hosts = host.isnull()\n",
    "missing_hosts = missing_hosts[list_non_null_host_fields].any(axis=1)\n",
    "host = host.loc[(~missing_hosts).values, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-Xivw-ZYYNh"
   },
   "outputs": [],
   "source": [
    "host['host_id'].isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeTrmBMncDPx"
   },
   "source": [
    "We now want to determine what types we want for each column. Below we convert to ints and strings. We also convert the percent entries into ints. We also convert 'host_is_superhost' into a boolean. Additionally we verify that 'host_response_time' is prepped to become an enum of type {'a few days or more', 'within a day', 'within a few hours', 'within an hour'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZJElb-dRZuK"
   },
   "outputs": [],
   "source": [
    "def perc_to_int(x): \n",
    "  return int(x[:-1])\n",
    "#int(host.loc[500]['host_response_rate'][:-1])\n",
    "\n",
    "# here we are converting the percentages to ints without the '%'\n",
    "host['host_response_rate'] = host['host_response_rate'].apply(lambda x : int(x[:-1]))\n",
    "host['host_acceptance_rate'] = host['host_acceptance_rate'].apply(lambda x : int(x[:-1]))\n",
    "\n",
    "# here we are converting the number of listings to ints\n",
    "host['host_total_listings_count'] = host['host_total_listings_count'].apply(lambda x : int(x))\n",
    "\n",
    "# here we will convert to strings \n",
    "convert_to_str = ['host_url','host_name','host_location','host_about','host_neighbourhood']\n",
    "for col in convert_to_str:  \n",
    "  host[col] = host[col].apply(lambda x : str(x))\n",
    "\n",
    "#host['host_url'] = host['host_url'].apply(lambda x : str(x))\n",
    "#host['host_name'] = host['host_name'].apply(lambda x : str(x))\n",
    "#host['host_location'] = host['host_location'].apply(lambda x : str(x))\n",
    "#host['host_about'] = host['host_about'].apply(lambda x : str(x))\n",
    "#host['host_neighbourhood'] = host['host_neighbourhood'].apply(lambda x : str(x))\n",
    "\n",
    "# 'Host_response_time' is already an enum :\n",
    "set(host['host_response_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pni42NvgakH7"
   },
   "outputs": [],
   "source": [
    "# make 'host_is_superhost' a boolean\n",
    "host['host_is_superhost'] = host['host_is_superhost'].apply(lambda x : False if x == 'f' else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlV-fmT5bXyx"
   },
   "outputs": [],
   "source": [
    "host.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWnwFPZ0dqNQ"
   },
   "source": [
    "#Cleaning Airbnb\n",
    "\n",
    "here we will remove nulls from the columns where we deem them not appropriate. \n",
    "\n",
    "We will also rename 2 columns to 'neighbourhood' and 'borough'\n",
    "\n",
    "We will then change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xTHcfQQdw-I"
   },
   "outputs": [],
   "source": [
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_fAapcUi5-f"
   },
   "outputs": [],
   "source": [
    "#rename columns 'neighbourhood_cleansed', 'neighbourhood_group_cleansed'\n",
    "airbnb = airbnb.rename(columns={'neighbourhood_cleansed': 'neighborhood', 'neighbourhood_group_cleansed': 'borough', 'bathrooms_text': 'bathrooms_details'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_zUiGelesyv"
   },
   "outputs": [],
   "source": [
    "list_non_null_airbnb_fields = ['id', 'listing_url', 'name', 'picture_url', 'host_id',\n",
    "'neighborhood', 'borough', 'latitude',\n",
    "'longitude', 'property_type', 'room_type', 'accommodates',\n",
    "'bathrooms_details', 'bedrooms', 'beds', 'amenities', 'price',\n",
    "'minimum_nights', 'maximum_nights', 'number_of_reviews', 'review_scores_rating',\n",
    "'review_scores_cleanliness', 'review_scores_checkin',\n",
    "'review_scores_communication', 'review_scores_location', 'instant_bookable']\n",
    "missing_airbnb = airbnb.isnull()\n",
    "missing_airbnb = missing_airbnb[list_non_null_airbnb_fields].any(axis=1)\n",
    "airbnb.size #1033844\n",
    "airbnb = airbnb.loc[(~missing_airbnb).values, :]\n",
    "airbnb.size #669284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FuTUhi3ra_7"
   },
   "outputs": [],
   "source": [
    "# making a dict to convert bathroom input to ints \n",
    "bathroom_dict = {'0 baths': 0,\n",
    " '0 shared baths': 0,\n",
    " '1 bath' : 1,\n",
    " '1 private bath': 1,\n",
    " '1 shared bath': 1,\n",
    " '1.5 baths': 1.5,\n",
    " '1.5 shared baths': 1.5,\n",
    " '15.5 baths': 15.5,\n",
    " '2 baths': 2,\n",
    " '2 shared baths': 2,\n",
    " '2.5 baths': 2.5,\n",
    " '2.5 shared baths': 2.5,\n",
    " '3 baths': 3,\n",
    " '3 shared baths': 3,\n",
    " '3.5 baths': 3.5,\n",
    " '3.5 shared baths': 3.5,\n",
    " '4 baths': 4,\n",
    " '4 shared baths': 4,\n",
    " '4.5 baths': 4.5,\n",
    " '5 baths': 5,\n",
    " '5.5 baths': 5.5,\n",
    " '6 baths': 6,\n",
    " '6 shared baths': 6,\n",
    " '6.5 baths': 6.5,\n",
    " '7 baths': 7,\n",
    " '8 baths': 8,\n",
    " 'Half-bath': 0.5,\n",
    " 'Private half-bath': 0.5,\n",
    " 'Shared half-bath': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cY-mw3l8h_P4"
   },
   "outputs": [],
   "source": [
    "# convert to floats\n",
    "#'id', 'host_id', 'amenities', 'price', 'instant_bookable'\n",
    "convert_to_float = ['review_scores_rating',\n",
    "'review_scores_cleanliness', 'review_scores_checkin',\n",
    "'review_scores_communication', 'latitude',\n",
    "'longitude', 'review_scores_location']\n",
    "convert_to_int = ['minimum_nights', 'maximum_nights', 'number_of_reviews', 'bedrooms', 'beds', 'accommodates', 'id', 'host_id']\n",
    "convert_to_str = ['listing_url', 'name', 'description',\n",
    "'neighborhood_overview', 'picture_url', 'neighborhood', 'property_type', 'bathrooms_details']\n",
    "\n",
    "for col in convert_to_float:  \n",
    "  airbnb[col] = airbnb[col].apply(lambda x : float(x))\n",
    "for col in convert_to_int:  \n",
    "  airbnb[col] = airbnb[col].apply(lambda x : int(x))\n",
    "for col in convert_to_str:  \n",
    "  airbnb[col] = airbnb[col].apply(lambda x : str(x))\n",
    "\n",
    "# here we are converting the prices to floats without the '$'\n",
    "airbnb['price'] = airbnb['price'].apply(lambda x : float(x[1:].replace(',','')))\n",
    "\n",
    "# here we are converting the bathrooms_text to just an int\n",
    "airbnb['bathrooms'] = airbnb['bathrooms_details'].apply(lambda x : bathroom_dict[x])\n",
    "\n",
    "# 'room_type' is already an enum :\n",
    "# {'Entire home/apt', 'Hotel room', 'Private room', 'Shared room'}\n",
    "set(airbnb['room_type'])\n",
    "# 'borough' is already an enum :\n",
    "# {'Bronx', 'Brooklyn', 'Manhattan', 'Queens', 'Staten Island'}\n",
    "set(airbnb['borough'])\n",
    "\n",
    "# make 'instant_bookable' a boolean\n",
    "airbnb['instant_bookable'] = airbnb['instant_bookable'].apply(lambda x : False if x == 'f' else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7sWDIH3iy0_"
   },
   "outputs": [],
   "source": [
    "set(airbnb['instant_bookable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWHuKUgHqDPn"
   },
   "outputs": [],
   "source": [
    "host.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgszVVaKt7CE"
   },
   "source": [
    "We finish with the following for **airbnb**\n",
    "```\n",
    "id                               int64 PRIMARY KEY\n",
    "listing_url                     object\n",
    "name                            object\n",
    "description                     object CAN BE NULL\n",
    "neighborhood_overview           object CAN BE NULL\n",
    "picture_url                     object\n",
    "host_id                          int64 FOREIGN KEY to Host\n",
    "neighborhood                    object\n",
    "borough                         object ENUM\n",
    "latitude                       float64\n",
    "longitude                      float64\n",
    "property_type                   object\n",
    "room_type                       object ENUM\n",
    "accommodates                     int64\n",
    "bathrooms_details               object\n",
    "bedrooms                         int64\n",
    "beds                             int64\n",
    "amenities                       object\n",
    "price                          float64\n",
    "minimum_nights                   int64\n",
    "maximum_nights                   int64\n",
    "number_of_reviews                int64\n",
    "review_scores_rating           float64\n",
    "review_scores_cleanliness      float64\n",
    "review_scores_checkin          float64\n",
    "review_scores_communication    float64\n",
    "review_scores_location         float64\n",
    "instant_bookable                  bool\n",
    "bathrooms                      float64\n",
    "```\n",
    "We finish with the following for **host**\n",
    "\n",
    "\n",
    "```\n",
    "host_id                       int64 PRIMARY KEY\n",
    "host_url                     object\n",
    "host_name                    object\n",
    "host_since                   object\n",
    "host_location                object\n",
    "host_about                   object CAN BE NULL\n",
    "host_response_time           object ENUM\n",
    "host_response_rate            int64\n",
    "host_acceptance_rate          int64\n",
    "host_is_superhost              bool\n",
    "host_neighbourhood           object\n",
    "host_total_listings_count     int64\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98CCMF7osH_c"
   },
   "source": [
    "## Entity Resolution\n",
    "\n",
    "Now that we've handled missing values in both datasets, we turn our attention to performing entity resolution on the entities common to both. In this case, those common entities are countries.\n",
    "\n",
    "To perform entity resolution, we will:\n",
    "1. Determine whether both datasets use the same names to refer to all countries\n",
    "2. Edit the names in one dataset to match the other, if necessary\n",
    "\n",
    "In the general case, you may also need to detect when datasets refer to different entities using the same name/ID and disambiguate these references. This may happen when handling datasets that contain multiple people with the same name, for example. But we don't need to worry about it here because the names of countries are well-known and distict. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9hb43PAv2Xa"
   },
   "source": [
    "### Detect Inconsistent Names\n",
    "Let's compile a list of all country names in both datasets, then inspect it for repetitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qSUfNMexChR"
   },
   "source": [
    "First, we extract the unique names of countries from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GttZPEV4xGEu"
   },
   "outputs": [],
   "source": [
    "census_countries = census[\"native-country\"].unique()\n",
    "le_countries = le[\"country\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60IeS3yJyJD_"
   },
   "source": [
    "Now, we combine these into one set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n416xpP1yO85"
   },
   "outputs": [],
   "source": [
    "countries = set(census_countries.tolist() + le_countries.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhg4MRXCyrsf"
   },
   "source": [
    "Let's inspect the contents of the set. The output is automatically sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljqsQSoVy5PZ"
   },
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKmwYr4gzEmZ"
   },
   "source": [
    "Immediately, we see that many of the names have spaces prepended to them that appear to be preventing matches. Let's print the lists of names from each dataset separately to see where these are coming from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEzkEXHuzOv5"
   },
   "outputs": [],
   "source": [
    "print(\"Census Countries\")\n",
    "print(census_countries)\n",
    "print(\"\")\n",
    "print(\"LE Countries\")\n",
    "print(le_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG_chEiw2-js"
   },
   "source": [
    "The output above indicates the names with spaces are coming from the census dataset. So let's remove those spaces, recompile the list, and take look for countries that appear twice in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhC7d3s43wTg"
   },
   "outputs": [],
   "source": [
    "fixed_census_countries = list()\n",
    "for country in census_countries.tolist():\n",
    "  if country[0] == \" \":\n",
    "    fixed_census_countries.append(country[1:])\n",
    "countries = set(fixed_census_countries + le_countries.tolist())\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyw0H0PN7QCg"
   },
   "source": [
    "The output contains 9 countries with multiple names:\n",
    "\n",
    "*   \"United States of America\" and \"United-States\"\n",
    "*   \"Trinidad and Tobago\" and \"Trinidad&Tobago\"\n",
    "*   \"Dominican Republic\" and \"Dominican-Republic\"\n",
    "*   \"El-Salvador\" and \"El Salvador\"\n",
    "*   \"Columbia\" and \"Colombia\"\n",
    "*   \"Netherlands\" and \"Holand-Netherlands\"\n",
    "*   \"Laos\" and \"Lao People's Democratic Republic\"\n",
    "*   \"Iran\" and \"Iran (Islamic Republic of)\"\n",
    "*   \"Viet Nam\" and \"Vietnam\"\n",
    "\n",
    "Relatedly, the output contains an entry for the United Kingdom, as well as entries for Scotland and England, which are part of the UK. \n",
    "\n",
    "*Republic of Korea and the Democratic People's Republic of Korea are different countries. Congo and Democratic Republic of the Congo are also different.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBPPffqk-IkX"
   },
   "source": [
    "### Resolve Inconsistent Names\n",
    "Now, let's fix the problems the we uncovered in the previous step.\n",
    "\n",
    "First, we trim the whitespace off the start and end of the country names in both datasets to handle that space issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Q1oX0BKAArE"
   },
   "outputs": [],
   "source": [
    "census[\"native-country\"] = census[\"native-country\"].str.strip()\n",
    "le[\"country\"] = le[\"country\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19nOkPXuA2xC"
   },
   "source": [
    "For each country with multiple names, we need to purge one of the names from the dataset. We'll do this by replacing all usages of one name with the other. \n",
    "\n",
    "We'll also replace all substitute all usages Scotland and England for the U.K. This solution sacrifices some precision, but at least we avoid the incorrect conclusion that there's no valid counterpart for rows containing Scotland and England.  \n",
    "\n",
    "To accomplish this, we first create a `Series` of the countries with multiple names, where of each element is the name we'll purge, and the value is name we'll keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ff8F_8zDL_A"
   },
   "outputs": [],
   "source": [
    "name_map = {\n",
    "     \"United-States\": \"United States of America\",\n",
    "     \"Trinadad&Tobago\": \"Trinidad and Tobago\",\n",
    "     \"Dominican-Republic\": \"Dominican Republic\",\n",
    "     \"Columbia\" : \"Colombia\",\n",
    "     \"Lao People's Democratic Republic\": \"Laos\",\n",
    "     \"Scotland\": \"United Kingdom of Great Britain and Northern Ireland\",\n",
    "     \"England\": \"United Kingdom of Great Britain and Northern Ireland\",\n",
    "     \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "     \"Holand-Netherlands\": \"Netherlands\",\n",
    "     \"Viet Nam\" : \"Vietnam\",\n",
    "     \"El-Salvador\": \"El Salvador\"\n",
    "}\n",
    "name_series = pd.Series(data=list(name_map.values()), index=list(name_map.keys()))\n",
    "name_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptKLCctdD3JQ"
   },
   "source": [
    "Next, we find the indices of all usages of the names we're purging in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HyFc3AgD2U_"
   },
   "outputs": [],
   "source": [
    "purge_list = list(name_map.keys())\n",
    "census_idx = census.index[census[\"native-country\"].isin(purge_list)]\n",
    "le_idx = le.index[le[\"country\"].isin(purge_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DpY9tzWEQTk"
   },
   "source": [
    "Using these indices, we extract the names that we need to update as `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3RXvDbBEfnh"
   },
   "outputs": [],
   "source": [
    "census_problems = census.loc[census_idx, \"native-country\"]\n",
    "le_problems = le.loc[le_idx, \"country\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1PlMNNhF6Il"
   },
   "source": [
    "Next, we replace the problematic names in these `Series` with the appropriate counterparts by indexing `name_series` with them. \n",
    "\n",
    "*The operations in the cell below replace the names correctly because `name_series` maps each name we wanted to purge to the we wanted to replace it with.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJw-v1HrE14w"
   },
   "outputs": [],
   "source": [
    "census_fixed = name_series.loc[census_problems.values].values\n",
    "le_fixed = name_series.loc[le_problems.values].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u8jJZzjH9MM"
   },
   "source": [
    "Finally, we update the dataframes with the fixed names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkQKesqmICgE"
   },
   "outputs": [],
   "source": [
    "census.loc[census_idx, \"native-country\"] = census_fixed\n",
    "le.loc[le_idx, \"country\"] = le_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QL3WaoCICHp"
   },
   "source": [
    "### Remove Unpaired Entities\n",
    "In some cases, you may want to exclude entities that only appear in one dataset or the other from your database. Let's suppose that's the case here and remove all countries that only appear in one dataset or the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqNfzN7tJL7U"
   },
   "source": [
    "As before, we first extract the full list of countries found in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr0C2XkcJcAW"
   },
   "outputs": [],
   "source": [
    "census_countries = census[\"native-country\"].unique().tolist()\n",
    "le_countries = le[\"country\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auiotb7RJhQc"
   },
   "source": [
    "Then, we convert these lists to sets and use set-difference operations to find the countries that only appear in one set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec2dYksZJsQa"
   },
   "outputs": [],
   "source": [
    "census_countries = set(census_countries)\n",
    "le_countries = set(le_countries)\n",
    "cen_diff = census_countries.difference(le_countries)\n",
    "le_diff = le_countries.difference(census_countries)\n",
    "total_diff = list(cen_diff) + list(le_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_X1tJo6LPBIY"
   },
   "source": [
    "Finally, we drop all rows from both datasets that contain countries that match any of the countries in this list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Bx25wbPLQg"
   },
   "outputs": [],
   "source": [
    "le = le.loc[~le[\"country\"].isin(total_diff), :]\n",
    "census = census.loc[~census[\"native-country\"].isin(total_diff), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MZVmgiXRqv5"
   },
   "source": [
    "## Replace Categorical Variables with Indicators\n",
    "\n",
    "Depending on your application, you may want to replace some of your text-based categorical variables with numeric equivalents. This substitution can reduce the runtime of some queries and decrease the your tables require. It can be especially fruitful for columns you plan to use as indices.  \n",
    "\n",
    "Let's see an example. Below, we'll convert the columns containing countries in our datasets to numeric equivalents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8NVf1uoTxSz"
   },
   "source": [
    "First, we create a `Series` that maps each country name to an integer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkg6Kst9T4Hd"
   },
   "outputs": [],
   "source": [
    "all_countries = le[\"country\"].unique()\n",
    "country_codes = pd.Series(index=all_countries, data=np.arange(len(all_countries)))\n",
    "country_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhH-RBVjUI7c"
   },
   "source": [
    "Next, we use this `Series` to map all countries in both datasets to the corresponding integers by indexing the `Series` with the names of the countries in the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi5HamRVUo3f"
   },
   "outputs": [],
   "source": [
    "le[\"country\"] = country_codes.loc[le[\"country\"]].values\n",
    "census[\"native-country\"] = country_codes.loc[census[\"native-country\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB_y0zbWVET5"
   },
   "source": [
    "Finally, we convert the columns to integer types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNgBAFypVZn3"
   },
   "outputs": [],
   "source": [
    "le[\"country\"] = le[\"country\"].astype(np.int64)\n",
    "census[\"native-country\"] = census[\"native-country\"].astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIKECQ-8a2y4"
   },
   "source": [
    "## Find an Index\n",
    "\n",
    "Before ingesting our data into the database, we need to find a unique index for each table. Let's do this for the life expectancy dataset. \n",
    "\n",
    "### Single Column Index\n",
    "The fastest way to determine whether any single column is unique is to check whether the number of unique values in the candidate column equals the number of elements. For example, let's find out whether `country` could be the index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpZM9Jr5b7rb"
   },
   "outputs": [],
   "source": [
    "len(le[\"country\"].unique()) == len(le[\"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBjWVulycIi2"
   },
   "source": [
    "### Multi-Column Index\n",
    "\n",
    "When there's not an individual column that can act as an index, we search for combinations of columns that can make a unique index when combined. \n",
    "\n",
    "To check a candidate set of columns:\n",
    "1. Call `DataFrame.groupby()` on the list of candidate columns. *This creates a group of rows for each unique combination of candidate olumn values that appears in the Dataframe*\n",
    "2. Call `GroupBy.size()` on the resulting grouped dataframe. *This counts the number of rows in each group*\n",
    "3. Check whether every group has exactly 1 row. \n",
    "\n",
    "We use this procedure below to check whether `country` and `year` can function as a joint index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHUnR5_1ee9D"
   },
   "outputs": [],
   "source": [
    "# Step 1: Group dataframe by candidate columns \n",
    "grouped_le = le.groupby([\"country\", \"year\"]) \n",
    "# Step 2: Count rows in each group\n",
    "counts = grouped_le.size()\n",
    "print(counts)\n",
    "# Step 3: Check whether every value equals 1\n",
    "(counts == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zObIej43fEXV"
   },
   "source": [
    "Great! This means no country-year pair appears more than once in the life expectancy table, so we can use `country` and `year` in combination as the index. \n",
    "\n",
    "For the census dataset, you'd need every column to create a unique index (Check this for yourself). So we'll just plan to use the arbitrary, unique integers `census.index` as our table index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpNzpcioVZVW"
   },
   "source": [
    "## Export Data\n",
    "After cleaning our datasets, resolving entity resolution problems, and choosing indices, we're ready to ingest our the data into our database.\n",
    "\n",
    "In the past, students have found using Python for data ingestion slow and frustrating, so we won't populate the database here. Instead, we'll export both datasets and the country codes to CSVs. Then, we'll show you how to ingest these CSVs into your database using MySQL Workbench in the tutorial on data ingestion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p0w9RrcXhWw"
   },
   "source": [
    "First, convert the `Series` of country codes to an equivalent `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyBLaDbPYron"
   },
   "outputs": [],
   "source": [
    "country_codes = pd.DataFrame(data={'country': country_codes.index, 'code': country_codes.values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzRipkHDY25m"
   },
   "source": [
    "Next, we use `DataFrame.to_csv` to write each dataset to a CSV file with a descriptive name. \n",
    "\n",
    "*For `le` and `country_codes`, we set `index` to `False` because the indices of the `DataFrames` are meaningless integers that we don't need in our tables. For `census`, we rename the index and include it in the output because we decided  to use it as our table index, even though it's arbitrary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IxY6b2bgNU9"
   },
   "outputs": [],
   "source": [
    "le.to_csv(\"le.csv\", index=False)\n",
    "country_codes.to_csv(\"country_codes.csv\", index=False)\n",
    "census.index.name = \"id\"\n",
    "census.to_csv(\"census.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ze_Ryi19ga7m"
   },
   "source": [
    "Finally, we download these files to our local machine, so we can put them into MySQL Workbench later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyC1g_Xsgfmm"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('census.csv') \n",
    "files.download(\"country_codes.csv\")\n",
    "files.download(\"le.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FavIv7NIC2_I"
   },
   "source": [
    "## Exercises\n",
    "Check out [these exercises](https://drive.google.com/open?id=1kjLaYC_KJUlltm-iAzgF5VmHgb7Uer0z) to practice the processing techniques you learned above!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Cozy with the Good Eats -- Data Cleaning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
